{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62211f72-c51f-4eee-930c-98a09a8e68e4",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "In this notebook, we use a new dataset, which includesthree new columns:\n",
    "1. sentiment_score and compound_sentiment are two kinds of sentiment score generated from text\n",
    "2. topic_list comes from topic modeling result. Each text is assigned with one topic, and we select the top5 words for that topic.\n",
    "\n",
    "We will use these two new features to see if they can improve our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6cf814-0273-4fe1-9522-dbfb9918d556",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28fab362-142c-41c7-9f77-59b2f1dd599f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dca4e5-cb94-4b2a-bd90-92624d841dd2",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4febc2-e0f7-42ad-bab8-fb87874e95d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>topic_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[releas, trauma, earthquak, sever, issu]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "\n",
       "   target  sentiment_score  compound_sentiment  \\\n",
       "0       1              0.0                 0.0   \n",
       "\n",
       "                                 topic_list  \n",
       "0  [releas, trauma, earthquak, sever, issu]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_excel(\"topic_model_sentiment.xlsx\")\n",
    "train_df['topic_list'] = train_df['topic_list'].apply(ast.literal_eval)\n",
    "train_df = train_df.drop('target', axis=1)\n",
    "column_mapping = {'target_relabelled': 'target'}\n",
    "train_df = train_df.rename(columns=column_mapping)\n",
    "train_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf2252-d9f2-4023-a399-692dcafa6342",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe709b75-8829-4a85-aada-56ce83808775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>topic_list</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[releas, trauma, earthquak, sever, issu]</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "\n",
       "   target  sentiment_score  compound_sentiment  \\\n",
       "0       1              0.0                 0.0   \n",
       "\n",
       "                                 topic_list  \\\n",
       "0  [releas, trauma, earthquak, sever, issu]   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "\n",
       "                           text_clean_string  \n",
       "0  deed reason earthquak may allah forgiv us  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_list = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def clean_text(text, LLM = False):\n",
    "    text = re.sub(r'https?://\\S+', '', text) # remove link\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)         # remove @mentions\n",
    "    text = re.sub(r'\\n',' ', text)           # remove line breaks\n",
    "    text = re.sub('\\s+', ' ', text).strip()  # remove leading, trailing, and extra spaces\n",
    "    text = re.sub(r'#', '', text)  # remove # from hashtag\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)#remove non-ASCII characters\n",
    "    if not LLM:\n",
    "        text = nltk.word_tokenize(text) # tokenize text\n",
    "        text = [t.lower() for t in text] # change words into lower case\n",
    "        text = [t for t in text if re.search('^[a-z]+$', t)] # only include alphabetic words\n",
    "        # text = [spell(t) for t in text]\n",
    "        text = [t for t in text if t not in stop_list] # remove stop words\n",
    "        text = [stemmer.stem(t) for t in text] # steeming words\n",
    "    return text\n",
    "\n",
    "def process_text(df, LLM = False):   \n",
    "    df1 = df.copy()\n",
    "    df1['text_clean'] = df1['text'].apply(lambda x: clean_text(x,LLM))\n",
    "    if LLM:        \n",
    "        return df1\n",
    "    else:\n",
    "        df1['text_clean_string'] = df1['text_clean'].apply(lambda x: \" \".join(x))\n",
    "        return df1\n",
    "\n",
    "train = process_text(train_df)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e726b7-8236-4bf2-95a0-ae50b5a2efbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deed reason earthquak may allah forgiv us\n",
      "deed reason earthquak may allah forgiv us releas trauma earthquak sever issu\n"
     ]
    }
   ],
   "source": [
    "# Append words in topic_list to text_clean_string\n",
    "# Define a custom function to concatenate values \n",
    "def concatenate_values(row):\n",
    "    return row['text_clean_string'] + ' ' + ' '.join(row['topic_list'])\n",
    "\n",
    "# Apply the custom function to create column C\n",
    "train['combined_string'] = train.apply(concatenate_values, axis=1)\n",
    "print(train.iloc[0].text_clean_string)\n",
    "print(train.iloc[0].combined_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79499304-e4c6-4414-ae4b-81bc1b2db241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>compound_sentiment</th>\n",
       "      <th>topic_list</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_clean_string</th>\n",
       "      <th>combined_string</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[releas, trauma, earthquak, sever, issu]</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>deed reason earthquak may allah forgiv us rele...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "\n",
       "   target  sentiment_score  compound_sentiment  \\\n",
       "0       1              0.0                 0.0   \n",
       "\n",
       "                                 topic_list  \\\n",
       "0  [releas, trauma, earthquak, sever, issu]   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "\n",
       "                           text_clean_string  \\\n",
       "0  deed reason earthquak may allah forgiv us   \n",
       "\n",
       "                                     combined_string  \n",
       "0  deed reason earthquak may allah forgiv us rele...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a2aa8-43b7-4b47-b937-ddeee7daea2b",
   "metadata": {},
   "source": [
    "### Word Frequency + Topic List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ec843be-6fab-4a7f-8f54-5858bb5de094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score on Training Set: 0.9544186046511628\n",
      "F1 Score on Test Set: 0.8096618357487922\n"
     ]
    }
   ],
   "source": [
    "X = train[\"combined_string\"]\n",
    "y = train['target'].to_list()\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train = count_vectorizer.fit_transform(X_train_df)\n",
    "LR = LogisticRegression(solver='newton-cg')\n",
    "LR.fit(X_train, y_train)\n",
    "X_test = count_vectorizer.transform(X_test_df)\n",
    "train_predictions = LR.predict(X_train)\n",
    "test_predictions = LR.predict(X_test)\n",
    "print(\"F1 Score on Training Set:\", f1_score(y_train, train_predictions))\n",
    "print(\"F1 Score on Test Set:\", f1_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f32750e-1951-42fc-8501-54cedac9a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CountVectorizer object\n",
    "pickle.dump(count_vectorizer, open(\"count_vectorizer.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d540e7fb-e90b-4334-9133-aa5de6a03b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the NumPy arrays to Python lists\n",
    "coef_list = LR.coef_.tolist()\n",
    "intercept_list = LR.intercept_.tolist()\n",
    "classes_list = LR.classes_.tolist()\n",
    "n_iter = LR.n_iter_\n",
    "\n",
    "# Create a custom JSON encoder to serialize the NumPy arrays\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "# Define the JSON dictionary\n",
    "json_dict = {\n",
    "    \"coef\": coef_list,\n",
    "    \"intercept\": intercept_list,\n",
    "    \"classes\": classes_list,\n",
    "    \"n_iter\": n_iter\n",
    "}\n",
    "\n",
    "# Save the model parameters to JSON using the custom encoder\n",
    "with open(\"best_model.json\", \"w\") as f:\n",
    "    json.dump(json_dict, f, indent=4, cls=NumpyEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f93dfaeb-98f5-446e-a3ab-69e1c15ac544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4913    beach read august giveaway hop amp give away b...\n",
       "2763    seen devast corp breach yet prepar get coverag...\n",
       "3364    roosevelt evacu order due wildfir fire traumat...\n",
       "3613    boy charg manslaught toddler report boy charg ...\n",
       "20          ridicul like thunderstorm sink video structur\n",
       "                              ...                        \n",
       "4292    prophet peac upon said hellfir even give half ...\n",
       "4817    differ moral system mine reject mass murder in...\n",
       "4737    lava blast power red pantherattack siren torna...\n",
       "1482    month payday short catastroph loan promot fina...\n",
       "5716    video pick bodi water rescuer search hundr mig...\n",
       "Name: combined_string, Length: 1523, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90e0dca1-4992-4aba-86d1-58525cb1529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_df.to_csv(\"X_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503c7b21-d03d-4f71-8f20-404c7b1009ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02499700-576b-4107-9ff1-9bc6e665ffd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889793c8-943f-4c35-b8e9-7c01d1208db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70be03b4-7bbf-447d-a47a-ddbb892a6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "y_pred = LR.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef3e2b1-57c2-4395-bb09-17ff2595ec2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Test Data  Prediction\n",
      "4913  beach read august giveaway hop amp give away b...           0\n",
      "2763  seen devast corp breach yet prepar get coverag...           0\n",
      "3364  roosevelt evacu order due wildfir fire traumat...           1\n",
      "3613  boy charg manslaught toddler report boy charg ...           1\n",
      "20        ridicul like thunderstorm sink video structur           0\n",
      "...                                                 ...         ...\n",
      "4292  prophet peac upon said hellfir even give half ...           0\n",
      "4817  differ moral system mine reject mass murder in...           1\n",
      "4737  lava blast power red pantherattack siren torna...           0\n",
      "1482  month payday short catastroph loan promot fina...           0\n",
      "5716  video pick bodi water rescuer search hundr mig...           1\n",
      "\n",
      "[1523 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "result_df = pd.DataFrame({'Test Data': X_test_df, 'Prediction': y_pred})\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e655ee02-fbcf-451c-b28b-0c4fca6c48d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ad83e6-02d2-4ff3-8aa3-da4a249c3123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
